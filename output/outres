learning rate: 0.010000
enable cuda is 1 device is cuda
SimpleNet(
  (features1): Sequential(
    (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU()
    (conv2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu2): ReLU()
    (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU()
  )
  (shortcut1): Sequential(
    (0): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))
    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (features2): Sequential(
    (conv4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu4): ReLU()
    (conv5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu5): ReLU()
    (conv6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu6): ReLU()
  )
  (shortcut2): Identity()
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (features3): Sequential(
    (conv7): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu7): ReLU()
  )
  (lin8): Linear(in_features=784, out_features=4096, bias=True)
  (relu8): ReLU()
  (lin9): Linear(in_features=4096, out_features=4096, bias=True)
  (relu9): ReLU()
  (lin10): Linear(in_features=4096, out_features=4096, bias=True)
  (relu10): ReLU()
  (lin11): Linear(in_features=4096, out_features=4096, bias=True)
  (relu11): ReLU()
  (lin12): Linear(in_features=4096, out_features=4096, bias=True)
  (relu12): ReLU()
  (lin): Linear(in_features=4096, out_features=10, bias=True)
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 4, 28, 28]              40
       BatchNorm2d-2            [-1, 4, 28, 28]               8
              ReLU-3            [-1, 4, 28, 28]               0
            Conv2d-4           [-1, 16, 28, 28]             592
       BatchNorm2d-5           [-1, 16, 28, 28]              32
              ReLU-6           [-1, 16, 28, 28]               0
            Conv2d-7           [-1, 16, 28, 28]           2,320
       BatchNorm2d-8           [-1, 16, 28, 28]              32
              ReLU-9           [-1, 16, 28, 28]               0
           Conv2d-10           [-1, 16, 28, 28]              32
      BatchNorm2d-11           [-1, 16, 28, 28]              32
        MaxPool2d-12           [-1, 16, 14, 14]               0
           Conv2d-13           [-1, 16, 14, 14]           2,320
      BatchNorm2d-14           [-1, 16, 14, 14]              32
             ReLU-15           [-1, 16, 14, 14]               0
           Conv2d-16           [-1, 16, 14, 14]           2,320
      BatchNorm2d-17           [-1, 16, 14, 14]              32
             ReLU-18           [-1, 16, 14, 14]               0
           Conv2d-19           [-1, 16, 14, 14]           2,320
      BatchNorm2d-20           [-1, 16, 14, 14]              32
             ReLU-21           [-1, 16, 14, 14]               0
         Identity-22           [-1, 16, 14, 14]               0
        MaxPool2d-23             [-1, 16, 7, 7]               0
           Conv2d-24             [-1, 16, 7, 7]           2,320
      BatchNorm2d-25             [-1, 16, 7, 7]              32
             ReLU-26             [-1, 16, 7, 7]               0
           Linear-27                 [-1, 4096]       3,215,360
             ReLU-28                 [-1, 4096]               0
           Linear-29                 [-1, 4096]      16,781,312
             ReLU-30                 [-1, 4096]               0
           Linear-31                 [-1, 4096]      16,781,312
             ReLU-32                 [-1, 4096]               0
           Linear-33                 [-1, 4096]      16,781,312
             ReLU-34                 [-1, 4096]               0
           Linear-35                 [-1, 4096]      16,781,312
             ReLU-36                 [-1, 4096]               0
           Linear-37                   [-1, 10]          40,970
================================================================
Total params: 70,394,074
Trainable params: 70,394,074
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 1.44
Params size (MB): 268.53
Estimated Total Size (MB): 269.97
----------------------------------------------------------------
Epoch: [ 1/ 10], Step: [ 100/ 600], Loss: 2.2941
Epoch: [ 1/ 10], Step: [ 200/ 600], Loss: 2.2891
Epoch: [ 1/ 10], Step: [ 300/ 600], Loss: 2.2694
Epoch: [ 1/ 10], Step: [ 400/ 600], Loss: 2.2344
Epoch: [ 1/ 10], Step: [ 500/ 600], Loss: 2.1441
Epoch: [ 1/ 10], Step: [ 600/ 600], Loss: 1.8409
part3.8 -- Epoch:[1/10], Loss: 1.8409
Epoch: [ 2/ 10], Step: [ 100/ 600], Loss: 1.2088
Epoch: [ 2/ 10], Step: [ 200/ 600], Loss: 0.8054
Epoch: [ 2/ 10], Step: [ 300/ 600], Loss: 0.5391
Epoch: [ 2/ 10], Step: [ 400/ 600], Loss: 0.2696
Epoch: [ 2/ 10], Step: [ 500/ 600], Loss: 0.1550
Epoch: [ 2/ 10], Step: [ 600/ 600], Loss: 0.2109
part3.8 -- Epoch:[2/10], Loss: 0.2109
Epoch: [ 3/ 10], Step: [ 100/ 600], Loss: 0.2654
Epoch: [ 3/ 10], Step: [ 200/ 600], Loss: 0.0912
Epoch: [ 3/ 10], Step: [ 300/ 600], Loss: 0.1055
Epoch: [ 3/ 10], Step: [ 400/ 600], Loss: 0.0709
Epoch: [ 3/ 10], Step: [ 500/ 600], Loss: 0.0881
Epoch: [ 3/ 10], Step: [ 600/ 600], Loss: 0.0556
part3.8 -- Epoch:[3/10], Loss: 0.0556
Epoch: [ 4/ 10], Step: [ 100/ 600], Loss: 0.0677
Epoch: [ 4/ 10], Step: [ 200/ 600], Loss: 0.0740
Epoch: [ 4/ 10], Step: [ 300/ 600], Loss: 0.0689
Epoch: [ 4/ 10], Step: [ 400/ 600], Loss: 0.0393
Epoch: [ 4/ 10], Step: [ 500/ 600], Loss: 0.1557
Epoch: [ 4/ 10], Step: [ 600/ 600], Loss: 0.0275
part3.8 -- Epoch:[4/10], Loss: 0.0275
Epoch: [ 5/ 10], Step: [ 100/ 600], Loss: 0.0529
Epoch: [ 5/ 10], Step: [ 200/ 600], Loss: 0.0358
Epoch: [ 5/ 10], Step: [ 300/ 600], Loss: 0.0170
Epoch: [ 5/ 10], Step: [ 400/ 600], Loss: 0.0749
Epoch: [ 5/ 10], Step: [ 500/ 600], Loss: 0.0309
Epoch: [ 5/ 10], Step: [ 600/ 600], Loss: 0.0395
part3.8 -- Epoch:[5/10], Loss: 0.0395
Epoch: [ 6/ 10], Step: [ 100/ 600], Loss: 0.0296
Epoch: [ 6/ 10], Step: [ 200/ 600], Loss: 0.0791
Epoch: [ 6/ 10], Step: [ 300/ 600], Loss: 0.0431
Epoch: [ 6/ 10], Step: [ 400/ 600], Loss: 0.0174
Epoch: [ 6/ 10], Step: [ 500/ 600], Loss: 0.0654
Epoch: [ 6/ 10], Step: [ 600/ 600], Loss: 0.0432
part3.8 -- Epoch:[6/10], Loss: 0.0432
Epoch: [ 7/ 10], Step: [ 100/ 600], Loss: 0.0218
Epoch: [ 7/ 10], Step: [ 200/ 600], Loss: 0.0166
Epoch: [ 7/ 10], Step: [ 300/ 600], Loss: 0.0277
Epoch: [ 7/ 10], Step: [ 400/ 600], Loss: 0.0385
Epoch: [ 7/ 10], Step: [ 500/ 600], Loss: 0.0193
Epoch: [ 7/ 10], Step: [ 600/ 600], Loss: 0.1001
part3.8 -- Epoch:[7/10], Loss: 0.1001
Epoch: [ 8/ 10], Step: [ 100/ 600], Loss: 0.0158
Epoch: [ 8/ 10], Step: [ 200/ 600], Loss: 0.0154
Epoch: [ 8/ 10], Step: [ 300/ 600], Loss: 0.0223
Epoch: [ 8/ 10], Step: [ 400/ 600], Loss: 0.0079
Epoch: [ 8/ 10], Step: [ 500/ 600], Loss: 0.0064
Epoch: [ 8/ 10], Step: [ 600/ 600], Loss: 0.0326
part3.8 -- Epoch:[8/10], Loss: 0.0326
Epoch: [ 9/ 10], Step: [ 100/ 600], Loss: 0.0662
Epoch: [ 9/ 10], Step: [ 200/ 600], Loss: 0.0515
Epoch: [ 9/ 10], Step: [ 300/ 600], Loss: 0.0216
Epoch: [ 9/ 10], Step: [ 400/ 600], Loss: 0.0393
Epoch: [ 9/ 10], Step: [ 500/ 600], Loss: 0.0703
Epoch: [ 9/ 10], Step: [ 600/ 600], Loss: 0.0095
part3.8 -- Epoch:[9/10], Loss: 0.0095
Epoch: [ 10/ 10], Step: [ 100/ 600], Loss: 0.0088
Epoch: [ 10/ 10], Step: [ 200/ 600], Loss: 0.0334
Epoch: [ 10/ 10], Step: [ 300/ 600], Loss: 0.0116
Epoch: [ 10/ 10], Step: [ 400/ 600], Loss: 0.0256
Epoch: [ 10/ 10], Step: [ 500/ 600], Loss: 0.0094
Epoch: [ 10/ 10], Step: [ 600/ 600], Loss: 0.0028
part3.8 -- Epoch:[10/10], Loss: 0.0028
Epoch:[1/10], accuracy: 98 %
Epoch:[2/10], accuracy: 98 %
Epoch:[3/10], accuracy: 98 %
Epoch:[4/10], accuracy: 98 %
Epoch:[5/10], accuracy: 98 %
Epoch:[6/10], accuracy: 98 %
Epoch:[7/10], accuracy: 98 %
Epoch:[8/10], accuracy: 98 %
Epoch:[9/10], accuracy: 98 %
Epoch:[10/10], accuracy: 98 %
total :10000, correct : 9891
Accuracy of the model on the 10000 test images:  98.909996 %
