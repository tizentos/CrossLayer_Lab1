learning rate: 0.001000
enable cuda is 1 device is cuda
SimpleNet(
  (features): Sequential(
    (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu1): ReLU()
    (conv2): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu2): ReLU()
    (conv3): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn3): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu3): ReLU()
    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv4): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu4): ReLU()
    (conv5): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu5): ReLU()
    (conv6): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn6): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu6): ReLU()
    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (conv7): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn7): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu7): ReLU()
    (conv8): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn8): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu8): ReLU()
    (conv9): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn9): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu9): ReLU()
    (conv10): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn10): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu10): ReLU()
    (conv11): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn11): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu11): ReLU()
    (conv12): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (bn12): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu12): ReLU()
  )
  (lin): Linear(in_features=784, out_features=10, bias=True)
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1            [-1, 4, 28, 28]              40
       BatchNorm2d-2            [-1, 4, 28, 28]               8
              ReLU-3            [-1, 4, 28, 28]               0
            Conv2d-4           [-1, 16, 28, 28]             592
       BatchNorm2d-5           [-1, 16, 28, 28]              32
              ReLU-6           [-1, 16, 28, 28]               0
            Conv2d-7           [-1, 16, 28, 28]           2,320
       BatchNorm2d-8           [-1, 16, 28, 28]              32
              ReLU-9           [-1, 16, 28, 28]               0
        MaxPool2d-10           [-1, 16, 14, 14]               0
           Conv2d-11           [-1, 16, 14, 14]           2,320
      BatchNorm2d-12           [-1, 16, 14, 14]              32
             ReLU-13           [-1, 16, 14, 14]               0
           Conv2d-14           [-1, 16, 14, 14]           2,320
      BatchNorm2d-15           [-1, 16, 14, 14]              32
             ReLU-16           [-1, 16, 14, 14]               0
           Conv2d-17           [-1, 16, 14, 14]           2,320
      BatchNorm2d-18           [-1, 16, 14, 14]              32
             ReLU-19           [-1, 16, 14, 14]               0
        MaxPool2d-20             [-1, 16, 7, 7]               0
           Conv2d-21             [-1, 16, 7, 7]           2,320
      BatchNorm2d-22             [-1, 16, 7, 7]              32
             ReLU-23             [-1, 16, 7, 7]               0
           Conv2d-24             [-1, 16, 7, 7]           2,320
      BatchNorm2d-25             [-1, 16, 7, 7]              32
             ReLU-26             [-1, 16, 7, 7]               0
           Conv2d-27             [-1, 16, 7, 7]           2,320
      BatchNorm2d-28             [-1, 16, 7, 7]              32
             ReLU-29             [-1, 16, 7, 7]               0
           Conv2d-30             [-1, 16, 7, 7]           2,320
      BatchNorm2d-31             [-1, 16, 7, 7]              32
             ReLU-32             [-1, 16, 7, 7]               0
           Conv2d-33             [-1, 16, 7, 7]           2,320
      BatchNorm2d-34             [-1, 16, 7, 7]              32
             ReLU-35             [-1, 16, 7, 7]               0
           Conv2d-36             [-1, 16, 7, 7]           2,320
      BatchNorm2d-37             [-1, 16, 7, 7]              32
             ReLU-38             [-1, 16, 7, 7]               0
           Linear-39                   [-1, 10]           7,850
================================================================
Total params: 32,042
Trainable params: 32,042
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.00
Forward/backward pass size (MB): 1.00
Params size (MB): 0.12
Estimated Total Size (MB): 1.12
----------------------------------------------------------------
Epoch: [ 1/ 10], Step: [ 100/ 600], Loss: 1.7816
Epoch: [ 1/ 10], Step: [ 200/ 600], Loss: 1.4332
Epoch: [ 1/ 10], Step: [ 300/ 600], Loss: 1.0714
Epoch: [ 1/ 10], Step: [ 400/ 600], Loss: 0.8671
Epoch: [ 1/ 10], Step: [ 500/ 600], Loss: 0.6591
Epoch: [ 1/ 10], Step: [ 600/ 600], Loss: 0.6139
part3.8 -- Epoch:[1/10], Loss: 0.6139
Epoch: [ 2/ 10], Step: [ 100/ 600], Loss: 0.4820
Epoch: [ 2/ 10], Step: [ 200/ 600], Loss: 0.3833
Epoch: [ 2/ 10], Step: [ 300/ 600], Loss: 0.4474
Epoch: [ 2/ 10], Step: [ 400/ 600], Loss: 0.3455
Epoch: [ 2/ 10], Step: [ 500/ 600], Loss: 0.3424
Epoch: [ 2/ 10], Step: [ 600/ 600], Loss: 0.3604
part3.8 -- Epoch:[2/10], Loss: 0.3604
Epoch: [ 3/ 10], Step: [ 100/ 600], Loss: 0.1702
Epoch: [ 3/ 10], Step: [ 200/ 600], Loss: 0.2053
Epoch: [ 3/ 10], Step: [ 300/ 600], Loss: 0.2358
Epoch: [ 3/ 10], Step: [ 400/ 600], Loss: 0.1660
Epoch: [ 3/ 10], Step: [ 500/ 600], Loss: 0.2599
Epoch: [ 3/ 10], Step: [ 600/ 600], Loss: 0.2982
part3.8 -- Epoch:[3/10], Loss: 0.2982
Epoch: [ 4/ 10], Step: [ 100/ 600], Loss: 0.1111
Epoch: [ 4/ 10], Step: [ 200/ 600], Loss: 0.2181
Epoch: [ 4/ 10], Step: [ 300/ 600], Loss: 0.1923
Epoch: [ 4/ 10], Step: [ 400/ 600], Loss: 0.1555
Epoch: [ 4/ 10], Step: [ 500/ 600], Loss: 0.1736
Epoch: [ 4/ 10], Step: [ 600/ 600], Loss: 0.1685
part3.8 -- Epoch:[4/10], Loss: 0.1685
Epoch: [ 5/ 10], Step: [ 100/ 600], Loss: 0.0946
Epoch: [ 5/ 10], Step: [ 200/ 600], Loss: 0.1380
Epoch: [ 5/ 10], Step: [ 300/ 600], Loss: 0.1900
Epoch: [ 5/ 10], Step: [ 400/ 600], Loss: 0.0988
Epoch: [ 5/ 10], Step: [ 500/ 600], Loss: 0.1399
Epoch: [ 5/ 10], Step: [ 600/ 600], Loss: 0.1315
part3.8 -- Epoch:[5/10], Loss: 0.1315
Epoch: [ 6/ 10], Step: [ 100/ 600], Loss: 0.1149
Epoch: [ 6/ 10], Step: [ 200/ 600], Loss: 0.1063
Epoch: [ 6/ 10], Step: [ 300/ 600], Loss: 0.1946
Epoch: [ 6/ 10], Step: [ 400/ 600], Loss: 0.1046
Epoch: [ 6/ 10], Step: [ 500/ 600], Loss: 0.1354
Epoch: [ 6/ 10], Step: [ 600/ 600], Loss: 0.1487
part3.8 -- Epoch:[6/10], Loss: 0.1487
Epoch: [ 7/ 10], Step: [ 100/ 600], Loss: 0.1323
Epoch: [ 7/ 10], Step: [ 200/ 600], Loss: 0.0741
Epoch: [ 7/ 10], Step: [ 300/ 600], Loss: 0.0850
Epoch: [ 7/ 10], Step: [ 400/ 600], Loss: 0.0879
Epoch: [ 7/ 10], Step: [ 500/ 600], Loss: 0.1013
Epoch: [ 7/ 10], Step: [ 600/ 600], Loss: 0.0666
part3.8 -- Epoch:[7/10], Loss: 0.0666
Epoch: [ 8/ 10], Step: [ 100/ 600], Loss: 0.1069
Epoch: [ 8/ 10], Step: [ 200/ 600], Loss: 0.0822
Epoch: [ 8/ 10], Step: [ 300/ 600], Loss: 0.1510
Epoch: [ 8/ 10], Step: [ 400/ 600], Loss: 0.0600
Epoch: [ 8/ 10], Step: [ 500/ 600], Loss: 0.0858
Epoch: [ 8/ 10], Step: [ 600/ 600], Loss: 0.1293
part3.8 -- Epoch:[8/10], Loss: 0.1293
Epoch: [ 9/ 10], Step: [ 100/ 600], Loss: 0.0668
Epoch: [ 9/ 10], Step: [ 200/ 600], Loss: 0.0899
Epoch: [ 9/ 10], Step: [ 300/ 600], Loss: 0.0425
Epoch: [ 9/ 10], Step: [ 400/ 600], Loss: 0.0996
Epoch: [ 9/ 10], Step: [ 500/ 600], Loss: 0.0440
Epoch: [ 9/ 10], Step: [ 600/ 600], Loss: 0.0696
part3.8 -- Epoch:[9/10], Loss: 0.0696
Epoch: [ 10/ 10], Step: [ 100/ 600], Loss: 0.0360
Epoch: [ 10/ 10], Step: [ 200/ 600], Loss: 0.0439
Epoch: [ 10/ 10], Step: [ 300/ 600], Loss: 0.1076
Epoch: [ 10/ 10], Step: [ 400/ 600], Loss: 0.0819
Epoch: [ 10/ 10], Step: [ 500/ 600], Loss: 0.0872
Epoch: [ 10/ 10], Step: [ 600/ 600], Loss: 0.0533
part3.8 -- Epoch:[10/10], Loss: 0.0533
Epoch:[1/10], accuracy: 97 %
Epoch:[2/10], accuracy: 96 %
Epoch:[3/10], accuracy: 96 %
Epoch:[4/10], accuracy: 96 %
Epoch:[5/10], accuracy: 96 %
Epoch:[6/10], accuracy: 97 %
Epoch:[7/10], accuracy: 97 %
Epoch:[8/10], accuracy: 97 %
Epoch:[9/10], accuracy: 97 %
Epoch:[10/10], accuracy: 98 %
total :10000, correct : 9803
Accuracy of the model on the 10000 test images:  98.029999 %
